{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from eunjeon import Mecab\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense, LSTM, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abuse_process_log_20220106.csv', 'apt_review_20220106.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"Data/\"\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 로딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 리뷰, 처리내역 로그"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hgnn = pd.read_csv(path + \"apt_review_20220106.csv\", encoding=\"UTF-8\")\n",
    "data_log = pd.read_csv(path + 'abuse_process_log_20220106.csv', encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 리뷰 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 처리내역 데이터 가공\n",
    "data_log = data_log[(data_log[\"type\"]==1)&(data_log[\"comment_id\"].isnull())] # Type == 1 : 아파트 리뷰\n",
    "data_log = data_log[~((data_log[\"reason\"].str.contains(\"무효\"))|(data_log[\"reason\"].str.contains(\"복원\")))]\n",
    "data_log = data_log[[\"review_id\", \"reason\"]].dropna().drop_duplicates(\"review_id\")\n",
    "\n",
    "# 병합\n",
    "data_hgnn = data_hgnn[[\"id\", \"content\", \"is_blocked\", \"is_deleted\", \"is_blinded\"]]\n",
    "data = pd.merge(data_hgnn, data_log.rename(columns={\"review_id\":\"id\"}), on=\"id\", how=\"left\")\n",
    "\n",
    "# 필터링\n",
    "data = data[~((data[\"is_deleted\"]==1)&(data[\"is_blocked\"]!=1)&(data[\"is_blinded\"]!=1))] # 자진삭제한 데이터\n",
    "data = data[~data[\"content\"].isnull()] # 내용이 기재되지 않은 데이터\n",
    "data = data[data[\"content\"].str.len() > 20] # 최소 20자 이상 리뷰\n",
    "\n",
    "# 스팸 기준: is_blocked = 1이거나, 처리사유가 기재된 데이터\n",
    "data[\"is_spam\"] = np.where((data[\"is_blocked\"]==1)|(~data[\"reason\"].isnull()), 1, 0)\n",
    "\n",
    "# 각 문장별, 1회 이상 스팸 처리된 문장은 is_spam에서 1, 나머지는 0으로 처리 (완전 중복문장 제거)\n",
    "data_input = data.groupby(\"content\").sum()[[\"is_spam\"]].reset_index()\n",
    "data_input[\"is_spam\"] = np.where(data_input[\"is_spam\"] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 키워드 형태소 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b099bbf65d634a4aa618ef50484399ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1993214.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tagger = Mecab()\n",
    "morphs = list()\n",
    "for i in tqdm(range(len(data_input))):\n",
    "    corpus = data_input[\"content\"].iloc[i]\n",
    "    try:\n",
    "        keywords = tagger.morphs(corpus.upper())\n",
    "        morphs.append([keywords])\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    \n",
    "    except:\n",
    "        morphs.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 단위로 띄어써서 재구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = pd.concat([pd.DataFrame(morphs)[0], data_input[\"is_spam\"].reset_index(drop=True)], axis=1).rename(columns={0:\"content\"}).dropna()\n",
    "data_input[\"content\"] = data_input[\"content\"].apply(lambda row: \" \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 후처리 (`zero with space` 삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input[\"content\"] = data_input[\"content\"].str.replace(\"\\u200b\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\b 지식 이 일정 수준 이상 이 며 , 다양 한 활동 경험 이 있 으신 분 들 이 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>목포 , 내항 , 남항 북항 서남권 경제 특화 항만 으로 개발 추진 HTTP : /...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>박상돈 천안시장 특별 대담 … 조정 지역 해제 건 의 시사 HTTPS : / / B...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>박상돈 천안시장 특별 대담 … 조정 지역 해제 건 의 시사 HTTP : / / WW...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>서연 이 음 터 도서관 화성시 장지동 115 - 1 대지면적 : 3 , 250 M ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993209</th>\n",
       "      <td>🤥 🤐 🤐 😯 🤢 😯 👹 😶 👿 😈 🤫 🤕🤢🎃🤕😺🤢🤢🐧🐴🙈🦇🐝🐵🦗🐸🪲🦋🙉🐮🐜🦋🐦🐛🙉🐜</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993210</th>\n",
       "      <td>🥈 🎇 🎎 🎁 🎳 🏉 🏆 🧧🏉🎐🏈🏏🏓🏑🏈🏒🧧🏈🎖🥈🎫🥈🥈🥎🥈🏀🥈🏅🥈🎑🥈🎖 ⚾ ️ 🏉 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993211</th>\n",
       "      <td>🥥🥦🌽🥔🥖🧈🍗🍟🍕🦴🦴🥞🤿🤺🥅🤾🏽 ‍♂ ️ 🤼 🪂 🧬 🛁 🩸 🪣 🩸 🧸 🪆 🕳 🛋 🇬...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993212</th>\n",
       "      <td>🥬 🥦 🫐 🥭 🍌 🍋🍊🍐🫑🥐🥨🏤🛖🏞🏞🧭🌐🧱🏢🏬🏯🏏🥎🎑🧨🎎🥉🎾🏏🏏</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993213</th>\n",
       "      <td>🫒🥚🍅🥖🧈🍗🧄🥒🍟🥒🦴🥓🥓🛹🛼🤼🎣🪃🤾🏽 ‍♂ ️ 🧬 🛎 🛀 🏿 🗝 🧸 🩸 🖼 🇬 🇺 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1993212 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   content  is_spam\n",
       "0        \b 지식 이 일정 수준 이상 이 며 , 다양 한 활동 경험 이 있 으신 분 들 이 ...        0\n",
       "1        목포 , 내항 , 남항 북항 서남권 경제 특화 항만 으로 개발 추진 HTTP : /...        0\n",
       "2        박상돈 천안시장 특별 대담 … 조정 지역 해제 건 의 시사 HTTPS : / / B...        0\n",
       "3        박상돈 천안시장 특별 대담 … 조정 지역 해제 건 의 시사 HTTP : / / WW...        0\n",
       "4        서연 이 음 터 도서관 화성시 장지동 115 - 1 대지면적 : 3 , 250 M ...        0\n",
       "...                                                    ...      ...\n",
       "1993209    🤥 🤐 🤐 😯 🤢 😯 👹 😶 👿 😈 🤫 🤕🤢🎃🤕😺🤢🤢🐧🐴🙈🦇🐝🐵🦗🐸🪲🦋🙉🐮🐜🦋🐦🐛🙉🐜        1\n",
       "1993210  🥈 🎇 🎎 🎁 🎳 🏉 🏆 🧧🏉🎐🏈🏏🏓🏑🏈🏒🧧🏈🎖🥈🎫🥈🥈🥎🥈🏀🥈🏅🥈🎑🥈🎖 ⚾ ️ 🏉 ...        1\n",
       "1993211  🥥🥦🌽🥔🥖🧈🍗🍟🍕🦴🦴🥞🤿🤺🥅🤾🏽 ‍♂ ️ 🤼 🪂 🧬 🛁 🩸 🪣 🩸 🧸 🪆 🕳 🛋 🇬...        1\n",
       "1993212                🥬 🥦 🫐 🥭 🍌 🍋🍊🍐🫑🥐🥨🏤🛖🏞🏞🧭🌐🧱🏢🏬🏯🏏🥎🎑🧨🎎🥉🎾🏏🏏        1\n",
       "1993213  🫒🥚🍅🥖🧈🍗🧄🥒🍟🥒🦴🥓🥓🛹🛼🤼🎣🪃🤾🏽 ‍♂ ️ 🧬 🛎 🛀 🏿 🗝 🧸 🩸 🖼 🇬 🇺 ...        1\n",
       "\n",
       "[1993212 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 학습용 데이터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X, y 세팅\n",
    "X = data_input[\"content\"]\n",
    "y = data_input[\"is_spam\"]\n",
    "\n",
    "# 학습-테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어집합 사이즈: 156192\n",
      "리뷰 시퀀스 최대 길이: 22771\n",
      "리뷰 시퀀스 평균 길이: 44.24576233452425\n",
      "훈련 데이터의 크기(shape): (1594569, 512)\n"
     ]
    }
   ],
   "source": [
    "# 리뷰 문장 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# 단어집합 총 크기\n",
    "word_to_index = tokenizer.word_index\n",
    "vocab_size = len(word_to_index) + 1\n",
    "print(f\"단어집합 사이즈: {vocab_size}\")\n",
    "print(f'리뷰 시퀀스 최대 길이: {max(len(l) for l in X_train_encoded)}')\n",
    "print(f'리뷰 시퀀스 평균 길이: {(sum(map(len, X_train_encoded))/len(X_train_encoded))}')\n",
    "\n",
    "# 훈련 데이터 패딩\n",
    "max_len = 512\n",
    "X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\n",
    "print(f\"훈련 데이터의 크기(shape): {X_train_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모형 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모형 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          9996288   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 10,008,737\n",
      "Trainable params: 10,008,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2336/2336 [==============================] - 239s 102ms/step - loss: 0.0996 - acc: 0.9685 - val_loss: 0.0874 - val_acc: 0.9704\n",
      "Epoch 2/2\n",
      "2336/2336 [==============================] - 239s 103ms/step - loss: 0.0789 - acc: 0.9722 - val_loss: 0.0854 - val_acc: 0.9710\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded, y_train, epochs=2, batch_size=512, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 정확도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12458/12458 [==============================] - 172s 14ms/step - loss: 0.0849 - acc: 0.9713\n",
      "테스트 정확도: 0.9712600111961365\n"
     ]
    }
   ],
   "source": [
    "X_test_encoded = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)\n",
    "print(f\"테스트 정확도: {model.evaluate(X_test_padded, y_test)[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습된 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(f\"Models/Spam_Classifier.h5\")\n",
    "with open(f'Models/Tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모형 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(f\"Models/Spam_Classifier.h5\")\n",
    "with open(f'Models/Tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separated(array):\n",
    "    return [\" \".join(tagger.morphs(array[i])) for i in range(len(array))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(array):\n",
    "    array = separated(list(array))\n",
    "    array = tokenizer.texts_to_sequences(array)\n",
    "    array = pad_sequences(array, maxlen = 512)\n",
    "    return model.predict(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터로 확률 예측\n",
    "data_test = pd.concat([X_test, y_test], axis=1)\n",
    "prob = predict(data_test[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분리 전 원본 데이터 복구\n",
    "data_test = pd.merge(data_test.reset_index().rename(columns={\"content\":\"content_edit\"}), \n",
    "                     data.groupby(\"content\").sum()[[\"is_spam\"]].reset_index().reset_index()[[\"index\", \"content\"]], \n",
    "                     on=\"index\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 데이터 정리\n",
    "result = pd.concat([data_test, pd.DataFrame(prob, columns=[\"prob\"])], axis=1)[[\"content\", \"content_edit\", \"is_spam\", \"prob\"]]\n",
    "result[\"verdict\"] = np.where(result[\"prob\"] > 0.5, 1, 0)\n",
    "result[\"prob\"] *= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_excel(\"리뷰별 스팸성 리뷰 예측결과.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
